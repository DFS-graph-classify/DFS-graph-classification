{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "graph_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4u1lj3MTNSLA",
        "19r0U7iK9YhQ",
        "Qh71qPti9PmG",
        "UsxEI-5zZPyi",
        "D8sKYu9UZYhm",
        "rGoYu9FQqwaG",
        "-oDyiiCwU36K",
        "wtUNI9-50o4V"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.18"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL38gv1n8OLW"
      },
      "source": [
        "### Mount drive \n",
        "##### (If using google colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfFQCH0-1Xxv"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbTf3yy--qbQ"
      },
      "source": [
        "%cd /content/drive/MyDrive/DFS-graph_classification/\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u1lj3MTNSLA"
      },
      "source": [
        "### Code for create graph and sequence \n",
        "##### (only need to run **once** to create graph and sequence for the dataset; if they already exists or created, **no need** to create again; if you wish to overwrite with new, you can run this again, and respond Y to delete existing)\n",
        "##### Need to set input arguments as desired for data processing in args.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvq1feD3LEQ2"
      },
      "source": [
        "!bash build.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyPde5VA_Nhk"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "import json\n",
        "from args import Args\n",
        "from utils import create_dirs\n",
        "from datasets.process_dataset import create_graphs\n",
        "from datasets.process_sequence import create_sequences\n",
        "from datasets.preprocess import calc_max_prev_node, dfscodes_weights\n",
        "seed = 7\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = Args()\n",
        "    args = args.update_args()\n",
        "\n",
        "    create_dirs(args)\n",
        "    \n",
        "    random.seed(seed)\n",
        "\n",
        "    #Create graphs and its min dfs code\n",
        "    graphs, create_graphs_time, dfscode_time = create_graphs(args)\n",
        "   \n",
        "    \n",
        "    print('Graph type:', args.graph_type)\n",
        "\n",
        "    # Loading the feature map\n",
        "    with open(args.current_dataset_path + 'label_0/map.dict', 'rb') as f:\n",
        "        feature_map = pickle.load(f)\n",
        "    print('\\n\\nInformation about Graphs of class 0: \\n')\n",
        "    print('Max number of nodes: {}'.format(feature_map['max_nodes']))\n",
        "    print('Max number of edges: {}'.format(feature_map['max_edges']))\n",
        "    print('Min number of nodes: {}'.format(feature_map['min_nodes']))\n",
        "    print('Min number of edges: {}'.format(feature_map['min_edges']))\n",
        "    print('Max degree of a node: {}'.format(feature_map['max_degree']))\n",
        "    print('No. of node labels: {}'.format(len(feature_map['node_forward'])))\n",
        "    print('No. of edge labels: {}'.format(len(feature_map['edge_forward'])))\n",
        "\n",
        "\n",
        "    with open(args.current_dataset_path + 'label_1/map.dict', 'rb') as f:\n",
        "        feature_map = pickle.load(f)\n",
        "    print('\\n\\nInformation about Graphs of class 1: \\n')\n",
        "    print('Max number of nodes: {}'.format(feature_map['max_nodes']))\n",
        "    print('Max number of edges: {}'.format(feature_map['max_edges']))\n",
        "    print('Min number of nodes: {}'.format(feature_map['min_nodes']))\n",
        "    print('Min number of edges: {}'.format(feature_map['min_edges']))\n",
        "    print('Max degree of a node: {}'.format(feature_map['max_degree']))\n",
        "    print('No. of node labels: {}'.format(len(feature_map['node_forward'])))\n",
        "    print('No. of edge labels: {}'.format(len(feature_map['edge_forward'])))\n",
        "\n",
        "    with open(args.current_dataset_path + 'all_graphs/map.dict', 'rb') as f:\n",
        "        feature_map = pickle.load(f)\n",
        "    print('\\n\\nInformation about All the Graphs: \\n')\n",
        "    print('Max number of nodes: {}'.format(feature_map['max_nodes']))\n",
        "    print('Max number of edges: {}'.format(feature_map['max_edges']))\n",
        "    print('Min number of nodes: {}'.format(feature_map['min_nodes']))\n",
        "    print('Min number of edges: {}'.format(feature_map['min_edges']))\n",
        "    print('Max degree of a node: {}'.format(feature_map['max_degree']))\n",
        "    print('No. of node labels: {}'.format(len(feature_map['node_forward'])))\n",
        "    print('No. of edge labels: {}'.format(len(feature_map['edge_forward'])))\n",
        "\n",
        "    #Create equences of min dfs code\n",
        "    create_sequences_time = create_sequences(args)\n",
        "\n",
        "    param_time = {\"create_graphs\" : create_graphs_time,\n",
        "                  \"dfscode\": dfscode_time,\n",
        "                  \"create_sequence\": create_sequences_time}\n",
        "                  \n",
        "    with open ('datasets/'+args.graph_type+'/time.txt', 'w') as f:\n",
        "        f.write(json.dumps(param_time, indent=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19r0U7iK9YhQ"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aczq8Uzljydk"
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# from bow import Vocabulary\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Embedding, LSTM, GRU, Bidirectional\n",
        "import keras\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "import random\n",
        "import csv\n",
        "seed = 7\n",
        "\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LGfVB7FqodM"
      },
      "source": [
        "### Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7XJxz8DFNng"
      },
      "source": [
        "#### Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHiVQCUfpaox"
      },
      "source": [
        "!mkdir results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7YRzLlIcMOt"
      },
      "source": [
        "dataset = 'MUTAG' #dataset name you wish to test on"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIkGNVdbwuUt"
      },
      "source": [
        "!mkdir results/{dataset}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh0Zk2dOtq88"
      },
      "source": [
        "edge_type = 'normal' #'normal' for orginal edge labels and 'default' for default edge labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PWW4Ih0VxSW"
      },
      "source": [
        "!mkdir results/{dataset}/{edge_type}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2njKhjAkZ10-"
      },
      "source": [
        "time_type = 'w' #'w' for with timestamp (min dfs code) and 'wo' for without timestamp (min dfs code variant)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lybrFztDDlsa"
      },
      "source": [
        "!mkdir results/{dataset}/{edge_type}/{time_type}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh71qPti9PmG"
      },
      "source": [
        "#### Function to create bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkU3RJ3Y80oR"
      },
      "source": [
        "from collections import Counter\n",
        "import json\n",
        "import numpy as np\n",
        " \n",
        "class Vocabulary:\n",
        " \n",
        "    def __init__(self, vocabulary, wordFrequencyFilePath):\n",
        "        self.vocabulary = vocabulary\n",
        "        self.WORD_FREQUENCY_FILE_FULL_PATH = wordFrequencyFilePath\n",
        "        self.input_word_index = {}\n",
        "        self.reverse_input_word_index = {}\n",
        " \n",
        "        self.MaxSentenceLength = None\n",
        " \n",
        "    def PrepareVocabulary(self, reviews):\n",
        "        self._prepare_Word_Frequency_Count_File(reviews)\n",
        "        self._create_Vocab_Indexes()\n",
        " \n",
        "        self.MaxSentenceLength = max([len(txt.split(\" \")) for txt in reviews])\n",
        " \n",
        "    def Get_Top_Words(self, number_words=None):\n",
        "        if number_words == None:\n",
        "            number_words = self.vocabulary\n",
        " \n",
        "        chars = json.loads(open(self.WORD_FREQUENCY_FILE_FULL_PATH).read())\n",
        "        counter = Counter(chars)\n",
        "        most_popular_words = {key for key,\n",
        "                              _value in counter.most_common(number_words)}\n",
        "        return most_popular_words\n",
        " \n",
        "    def _prepare_Word_Frequency_Count_File(self, reviews):\n",
        "        counter = Counter()\n",
        "        for s in reviews:\n",
        "            counter.update(s.split(\" \"))\n",
        " \n",
        "        with open(self.WORD_FREQUENCY_FILE_FULL_PATH, 'w') as output_file:\n",
        "            output_file.write(json.dumps(counter))\n",
        " \n",
        "    def _create_Vocab_Indexes(self):\n",
        "        INPUT_WORDS = self.Get_Top_Words(self.vocabulary)\n",
        " \n",
        "        for i, word in enumerate(INPUT_WORDS):\n",
        "            self.input_word_index[word] = i\n",
        " \n",
        "        for word, i in self.input_word_index.items():\n",
        "            self.reverse_input_word_index[i] = word\n",
        " \n",
        "    def TransformSentencesToId(self, sentences):\n",
        "        vectors = []\n",
        "        for r in sentences:\n",
        "            words = r.split(\" \")\n",
        "            vector = np.zeros(len(words))\n",
        " \n",
        "            for t, word in enumerate(words):\n",
        "                if word in self.input_word_index:\n",
        "                    vector[t] = self.input_word_index[word]\n",
        "                else:\n",
        "                    pass\n",
        " \n",
        "            vectors.append(vector)\n",
        " \n",
        "        return vectors\n",
        " \n",
        "    def ReverseTransformSentencesToId(self, sentences):\n",
        "        vectors = []\n",
        "        for r in sentences:\n",
        "            words = r.split(\" \")\n",
        "            vector = np.zeros(len(words))\n",
        " \n",
        "            for t, word in enumerate(words):\n",
        "                if word in self.input_word_index:\n",
        "                    vector[t] = self.input_word_index[word]\n",
        "                else:\n",
        "                    pass\n",
        "                    # vector[t] = 2 #unk\n",
        "            vectors.append(vector)\n",
        " \n",
        "        return vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsxEI-5zZPyi"
      },
      "source": [
        "#### with timestamp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A2iWScJqu-U"
      },
      "source": [
        "if time_type == 'w' and edge_type == 'normal':\n",
        "  path = 'datasets/'+dataset+'/sequences/with_timestamp/'\n",
        "\n",
        "  with open(path + 'label_1/with_timestamp_sequence_label_1.txt', 'r') as f:\n",
        "      reviews_positive = json.loads(f.read())\n",
        "\n",
        "  with open(path + 'label_0/with_timestamp_sequence_label_0.txt', 'r') as f:\n",
        "      reviews_negative = json.loads(f.read())\n",
        "\n",
        "  # print(reviews_negative)\n",
        "  if dataset == 'MUTAG':\n",
        "      TOP_WORDS = 372\n",
        "  elif dataset == 'PTC_FR':\n",
        "      TOP_WORDS = 1253\n",
        "  elif dataset == 'NCI-H23':\n",
        "      TOP_WORDS = 7260\n",
        "  elif dataset == 'TOX21_AR':\n",
        "      TOP_WORDS = 8780\n",
        "  elif dataset == 'DBLP_v1':\n",
        "      TOP_WORDS = 58184\n",
        "  else:\n",
        "      TOP_WORDS = 100000\n",
        "\n",
        "  print(dataset)\n",
        "  print(\"TOP WORDS: \",TOP_WORDS)\n",
        "\n",
        "  Reviews_Labeled = list(zip(reviews_positive, np.ones(len(reviews_positive))))\n",
        "  Reviews_Labeled.extend(\n",
        "      list(zip(reviews_negative, np.zeros(len(reviews_negative)))))\n",
        "  random.seed(7)\n",
        "  random.shuffle(Reviews_Labeled)\n",
        "  # print(Reviews_Labeled)\n",
        "  print(Reviews_Labeled)\n",
        "  vocab = Vocabulary(TOP_WORDS, path + \"analysis.vocab\")\n",
        "\n",
        "  reviews_text = [line[0] for line in Reviews_Labeled]\n",
        "  vocab.PrepareVocabulary(reviews_text)\n",
        "\n",
        "  with open(path + \"analysis.vocab\") as f:\n",
        "      data = json.load(f)\n",
        "      print('Total number of words: ' + str(len(data)))\n",
        "\n",
        "  reviews, labels = zip(*Reviews_Labeled)\n",
        "  reviews_int = vocab.TransformSentencesToId(reviews)\n",
        "\n",
        "  X = np.array(reviews_int, dtype=object)\n",
        "  max_len = max([len(i) for i in X])\n",
        "  print(\"Maximum length :\", max_len)\n",
        "  max_review_length = max_len\n",
        "  X = sequence.pad_sequences(X, maxlen=max_review_length)\n",
        "  Y = np.array(labels)\n",
        "  kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_xoUPwhte9u"
      },
      "source": [
        "if time_type == 'w' and edge_type == 'default':\n",
        "  path = '/content/drive/MyDrive/NTU_graph_classification/datasets/'+dataset+'/default/sequences-default/with_timestamp/'\n",
        "\n",
        "  with open(path + 'label_1/with_timestamp_sequence_label_1.txt', 'r') as f:\n",
        "      reviews_positive = json.loads(f.read())\n",
        "\n",
        "  with open(path + 'label_0/with_timestamp_sequence_label_0.txt', 'r') as f:\n",
        "      reviews_negative = json.loads(f.read())\n",
        "\n",
        "  # print(reviews_negative)\n",
        "  if dataset == 'MUTAG':\n",
        "      TOP_WORDS = 300\n",
        "  elif dataset == 'PTC_FR':\n",
        "      TOP_WORDS = 1018\n",
        "  elif dataset == 'NCI-H23':\n",
        "      TOP_WORDS = 4913\n",
        "  elif dataset == 'IMDB-BINARY':\n",
        "    TOP_WORDS = 23415\n",
        "  elif dataset == 'DBLP_v1':\n",
        "      TOP_WORDS = 57552\n",
        "  else:\n",
        "      TOP_WORDS = 100000\n",
        "\n",
        "  print(dataset)\n",
        "  print(\"TOP WORDS: \",TOP_WORDS)\n",
        "\n",
        "  Reviews_Labeled = list(zip(reviews_positive, np.ones(len(reviews_positive))))\n",
        "  Reviews_Labeled.extend(\n",
        "      list(zip(reviews_negative, np.zeros(len(reviews_negative)))))\n",
        "  random.seed(7)\n",
        "  random.shuffle(Reviews_Labeled)\n",
        "  print(Reviews_Labeled)\n",
        "  vocab = Vocabulary(TOP_WORDS, path + \"analysis.vocab\")\n",
        "\n",
        "  reviews_text = [line[0] for line in Reviews_Labeled]\n",
        "  vocab.PrepareVocabulary(reviews_text)\n",
        "\n",
        "  with open(path + \"analysis.vocab\") as f:\n",
        "      data = json.load(f)\n",
        "      print('Total number of words: ' + str(len(data)))\n",
        "\n",
        "  reviews, labels = zip(*Reviews_Labeled)\n",
        "  reviews_int = vocab.TransformSentencesToId(reviews)\n",
        "\n",
        "  X = np.array(reviews_int, dtype=object)\n",
        "  max_len = max([len(i) for i in X])\n",
        "  print(\"Maximum length :\", max_len)\n",
        "  max_review_length = max_len\n",
        "  X = sequence.pad_sequences(X, maxlen=max_review_length)\n",
        "  Y = np.array(labels)\n",
        "  kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0FSHyd2WVdC"
      },
      "source": [
        "if time_type == 'w': \n",
        "  # print(kfold)\n",
        "  fold_index = {\"train\": [], \"test\": []}\n",
        "  for train_index, test_index in kfold.split(X,Y):\n",
        "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = Y[train_index], Y[test_index]\n",
        "    fold_index[\"train\"].append(train_index.tolist())\n",
        "    fold_index[\"test\"].append(test_index.tolist())\n",
        "\n",
        "  print(fold_index['test'])\n",
        "  with open ('results/'+dataset+'/'+edge_type+'/w/indexes.txt', 'w') as f:\n",
        "      f.write(json.dumps(fold_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8sKYu9UZYhm"
      },
      "source": [
        "#### without timestamp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfk35beLWz7u"
      },
      "source": [
        "if time_type == 'wo' and edge_type == 'normal':\n",
        "  path = 'datasets/'+dataset+'/sequences/without_timestamp/'\n",
        "\n",
        "  with open(path + 'label_1/without_timestamp_sequence_label_1.txt', 'r') as f:\n",
        "      reviews_positive = json.loads(f.read())\n",
        "  with open(path + 'label_0/without_timestamp_sequence_label_0.txt', 'r') as f:\n",
        "      reviews_negative = json.loads(f.read())\n",
        "\n",
        "  if dataset == 'MUTAG':\n",
        "      TOP_WORDS = 23\n",
        "  elif dataset == 'PTC_FR':\n",
        "      TOP_WORDS = 70\n",
        "  elif dataset == 'NCI-H23':\n",
        "      TOP_WORDS = 80\n",
        "  elif dataset == 'TOX21_AR':\n",
        "      TOP_WORDS = 178\n",
        "  elif dataset == 'DBLP_v1':\n",
        "      TOP_WORDS = 47295\n",
        "  else :\n",
        "      TOP_WORDS = 2000\n",
        "\n",
        "  print(dataset)\n",
        "  print(\"TOP WORDS: \",TOP_WORDS)\n",
        "\n",
        "  Reviews_Labeled = list(zip(reviews_positive, np.ones(len(reviews_positive))))\n",
        "  Reviews_Labeled.extend(\n",
        "      list(zip(reviews_negative, np.zeros(len(reviews_negative)))))\n",
        "  random.seed(7)\n",
        "  random.shuffle(Reviews_Labeled)\n",
        "  print(Reviews_Labeled)\n",
        "  vocab = Vocabulary(TOP_WORDS, path + \"analysis.vocab\")\n",
        "\n",
        "  reviews_text = [line[0] for line in Reviews_Labeled]\n",
        "  vocab.PrepareVocabulary(reviews_text)\n",
        "\n",
        "  with open(path + \"analysis.vocab\") as f:\n",
        "      data = json.load(f)\n",
        "      print('Total number of words: ' + str(len(data)))\n",
        "\n",
        "  reviews, labels = zip(*Reviews_Labeled)\n",
        "  reviews_int = vocab.TransformSentencesToId(reviews)\n",
        "\n",
        "  X = np.array(reviews_int, dtype=object)\n",
        "  max_len = max([len(i) for i in X])\n",
        "  print(\"Maximum length :\", max_len)\n",
        "  max_review_length = max_len\n",
        "  X = sequence.pad_sequences(X, maxlen=max_review_length)\n",
        "  Y = np.array(labels)\n",
        "  kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo12E-t9vbA3"
      },
      "source": [
        "if time_type == 'wo' and edge_type == 'default': \n",
        "  path = '/content/drive/MyDrive/NTU_graph_classification/datasets/'+dataset+'/default/sequences-default/without_timestamp/'\n",
        "\n",
        "  with open(path + 'label_1/without_timestamp_sequence_label_1.txt', 'r') as f:\n",
        "      reviews_positive = json.loads(f.read())\n",
        "      # print(reviews_positive)\n",
        "  with open(path + 'label_0/without_timestamp_sequence_label_0.txt', 'r') as f:\n",
        "      reviews_negative = json.loads(f.read())\n",
        "\n",
        "  # print(reviews_negative)\n",
        "  if dataset == 'MUTAG':\n",
        "      TOP_WORDS = 12\n",
        "  elif dataset == 'PTC_FR':\n",
        "      TOP_WORDS = 41\n",
        "  elif dataset == 'NCI-H23':\n",
        "      TOP_WORDS = 54\n",
        "  elif dataset == 'IMDB-BINARY':\n",
        "      TOP_WORDS = 1615\n",
        "  elif dataset == 'DBLP_v1':\n",
        "      TOP_WORDS = 47217\n",
        "  else :\n",
        "      TOP_WORDS = 2000\n",
        "\n",
        "  print(dataset)\n",
        "  print(\"TOP WORDS: \",TOP_WORDS)\n",
        "\n",
        "  Reviews_Labeled = list(zip(reviews_positive, np.ones(len(reviews_positive))))\n",
        "  Reviews_Labeled.extend(\n",
        "      list(zip(reviews_negative, np.zeros(len(reviews_negative)))))\n",
        "  random.seed(7)\n",
        "  random.shuffle(Reviews_Labeled)\n",
        "  print(Reviews_Labeled)\n",
        "  vocab = Vocabulary(TOP_WORDS, path + \"analysis.vocab\")\n",
        "\n",
        "  reviews_text = [line[0] for line in Reviews_Labeled]\n",
        "  vocab.PrepareVocabulary(reviews_text)\n",
        "\n",
        "  with open(path + \"analysis.vocab\") as f:\n",
        "      data = json.load(f)\n",
        "      print('Total number of words: ' + str(len(data)))\n",
        "\n",
        "  reviews, labels = zip(*Reviews_Labeled)\n",
        "  reviews_int = vocab.TransformSentencesToId(reviews)\n",
        "\n",
        "  X = np.array(reviews_int, dtype=object)\n",
        "  max_len = max([len(i) for i in X])\n",
        "  print(\"Maximum length :\", max_len)\n",
        "  max_review_length = max_len\n",
        "  X = sequence.pad_sequences(X, maxlen=max_review_length)\n",
        "  Y = np.array(labels)\n",
        "  kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIUYnM86XU1w"
      },
      "source": [
        "if time_type == 'wo':\n",
        "  fold_index = {\"train\": [], \"test\": []}\n",
        "  for train_index, test_index in kfold.split(X,Y):\n",
        "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = Y[train_index], Y[test_index]\n",
        "    fold_index[\"train\"].append(train_index.tolist())\n",
        "    fold_index[\"test\"].append(test_index.tolist())\n",
        "\n",
        "  print(fold_index['test'])\n",
        "  with open ('results/'+dataset+'/'+edge_type+'/wo/indexes.txt', 'w') as f:\n",
        "      f.write(json.dumps(fold_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGoYu9FQqwaG"
      },
      "source": [
        "### CallBack function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6k2vcapj4cQ"
      },
      "source": [
        "\n",
        "# define your custom callback for prediction\n",
        "class PredictionCallback(tf.keras.callbacks.Callback): \n",
        "    def __init__(self, valx, file):\n",
        "        super().__init__()\n",
        "        self.validation_data = valx\n",
        "        self.filename = file\n",
        "        # self.batch_size = batch_size\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        with open(filename, 'w', newline='') as file:\n",
        "          writer = csv.writer(file)\n",
        "          writer.writerow([\"roc\", \"prc\", \"train loss\", \"train acc\", \"val loss\", \"val acc\", \"epoch time\"])\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.epoch_time_end = time.time() - self.epoch_time_start\n",
        "        yhat_probs = self.model.predict(self.validation_data[0])\n",
        "    \n",
        "        yhat_probs = yhat_probs[:, 0]\n",
        "          \n",
        "        roc_auc = metrics.roc_auc_score(self.validation_data[1], yhat_probs, average='macro')\n",
        "        print('ROC AUC: %f' % roc_auc)\n",
        "        \n",
        "        prc_auc = metrics.average_precision_score(\n",
        "            self.validation_data[1], yhat_probs, average='macro', pos_label=1)\n",
        "        print('PRC AUC: %f' % prc_auc)\n",
        "        \n",
        "        # Final evaluation of the models\n",
        "        scores = model.evaluate(self.validation_data[0], self.validation_data[1], verbose=0)\n",
        "        print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "        \n",
        "        with open(self.filename, 'a') as f_object:\n",
        "        \n",
        "            writer_object = csv.writer(f_object)\n",
        "            \n",
        "            writer_object.writerow([\"%.4f\" % roc_auc, \"%.4f\" % prc_auc,\"%.4f\" % logs['loss'], \"%.4f\" % (logs['accuracy']*100), \"%.4f\" % scores[0], \"%.4f\" % (scores[1]*100), \"%.5f\" % self.epoch_time_end])\n",
        "\n",
        "            f_object.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_gFcZvO0b9q"
      },
      "source": [
        "### Function to save averages and its standard deviation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuPSPsv70fHa"
      },
      "source": [
        "def save_avg(input_path):\n",
        "  avg_in = list()\n",
        "  directory = input_path\n",
        "  for filename in os.listdir(directory):\n",
        "          if filename.endswith(\".csv\") and filename != \"average.csv\" and filename != \"average_with_std.csv\":\n",
        "                  print(\"file: \",filename)\n",
        "                  df = pd.read_csv(directory+filename)\n",
        "                  print(df.to_numpy()[0][0])\n",
        "                  avg_in.append(df.to_numpy().tolist())\n",
        "\n",
        "  avg = np.mean(avg_in,axis=0)\n",
        "  avg_std = np.std(avg_in,axis=0)\n",
        "  save_file = directory + \"average_with_std.csv\"\n",
        "  header = [\"roc\", \"prc\",  \"train loss\", \"train acc\", \"val loss\", \"val acc\", \"epoch time\"]\n",
        "  header_std = [\"roc std\", \"prc std\", \"val acc std\"]\n",
        "  data = pd.DataFrame({header[0]: avg[:,0], header_std[0]:avg_std[:,0], header[1]: avg[:,1], header_std[1]:avg_std[:,1], header[2]: avg[:,2], header[3]: avg[:,3], header[4]: avg[:,4], header[5]: avg[:,5], header_std[2]:avg_std[:,5], header[6]: avg[:,6]})\n",
        "  data.to_csv(save_file, index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nISjVpWss-Ja"
      },
      "source": [
        "### Function to store top values "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA6jc3uqs-Jb"
      },
      "source": [
        "def top_values(directory, lr, filename_save, filename_save_approx):\n",
        "  filename = directory + \"lr_\"+ str(lr) + \"/average_with_std.csv\"\n",
        "  # filename = \"average_with_std.csv\"\n",
        "  data = pd.read_csv(filename)\n",
        "  data = data.to_numpy()\n",
        "  max_roc, max_prc, max_acc = 0, 0, 0\n",
        "  max_roc_i, max_prc_i, max_acc_i = 0, 0, 0\n",
        "  for i in range(len(data)):\n",
        "    # print(i)\n",
        "    if data[i][0]>max_roc :\n",
        "      max_roc = data[i][0]\n",
        "      max_roc_i = i\n",
        "\n",
        "    if data[i][2]>max_prc :\n",
        "      max_prc = data[i][2]\n",
        "      max_prc_i = i\n",
        "\n",
        "    if data[i][7]>max_acc :\n",
        "      max_acc = data[i][7]\n",
        "      max_acc_i = i\n",
        "\n",
        "  print(max_roc)\n",
        "  print(max_prc)\n",
        "  print(max_acc)\n",
        "\n",
        "  with open(filename_save, 'a') as f_object:\n",
        "      writer_object = csv.writer(f_object)\n",
        "      writer_object.writerow([lr, max_roc_i + 1, data[max_roc_i][0], data[max_roc_i][1], data[max_roc_i][2], data[max_roc_i][3], data[max_roc_i][7], data[max_roc_i][8], 64, 0])      \n",
        "      writer_object.writerow([lr, max_prc_i + 1, data[max_prc_i][0], data[max_prc_i][1], data[max_prc_i][2], data[max_prc_i][3], data[max_prc_i][7], data[max_prc_i][8], 64, 0])      \n",
        "      writer_object.writerow([lr, max_acc_i + 1, data[max_acc_i][0], data[max_acc_i][1], data[max_acc_i][2], data[max_acc_i][3], data[max_acc_i][7], data[max_acc_i][8], 64, 0])      \n",
        "      writer_object.writerow([\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"])\n",
        "      f_object.close()\n",
        "\n",
        "  with open(filename_save_approx, 'a') as f_object:\n",
        "      writer_object = csv.writer(f_object)\n",
        "      writer_object.writerow([lr, max_roc_i + 1, \"%.3f\" % data[max_roc_i][0], \"%.3f\" % data[max_roc_i][1], \"%.3f\" % data[max_roc_i][2], \"%.3f\" % data[max_roc_i][3], \"%.3f\" % data[max_roc_i][7], \"%.3f\" % data[max_roc_i][8], 64, 0])      \n",
        "      writer_object.writerow([lr, max_prc_i + 1, \"%.3f\" % data[max_prc_i][0], \"%.3f\" % data[max_prc_i][1], \"%.3f\" % data[max_prc_i][2], \"%.3f\" % data[max_prc_i][3], \"%.3f\" % data[max_prc_i][7], \"%.3f\" % data[max_prc_i][8], 64, 0])      \n",
        "      writer_object.writerow([lr, max_acc_i + 1, \"%.3f\" % data[max_acc_i][0], \"%.3f\" % data[max_acc_i][1], \"%.3f\" % data[max_acc_i][2], \"%.3f\" % data[max_acc_i][3], \"%.3f\" % data[max_acc_i][7], \"%.3f\" % data[max_acc_i][8], 64, 0])      \n",
        "      writer_object.writerow([\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"])\n",
        "      f_object.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBmQ5lR97cOI"
      },
      "source": [
        "### Code for LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMpV7Cc_V505"
      },
      "source": [
        "#### Make directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyBAXmlS-LpY"
      },
      "source": [
        "!mkdir results/{dataset}/{edge_type}/{time_type}/lstm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aby6ombjXT2"
      },
      "source": [
        "print('positive',len(reviews_positive))\n",
        "print('negative',len(reviews_negative))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyuYkIrwVu6l"
      },
      "source": [
        "#### Hyperparam setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aemVKjw75S4W"
      },
      "source": [
        "embedding_vector_length = 32\n",
        "epoch = 350\n",
        "batch_size = 64\n",
        "lr_arr = [0.01, 0.001, 0.0001]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOVpPPcrs-Jc"
      },
      "source": [
        "#### Setting to save top values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mENPxwt4s-Jc"
      },
      "source": [
        "directory = 'results/'+dataset+'/'+edge_type+'/'+time_type+'/lstm/'\n",
        "\n",
        "filename_save = directory + \"top_code.csv\"\n",
        "filename_save_approx = directory + \"top_code_rounded.csv\"\n",
        "\n",
        "with open(filename_save, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"lr\", \"epoch\", \"auc roc\", \"auc roc std\", \"auc prc\", \"auc prc std\", \"val acc\", \"val acc std\", \"batch size\" ,\"epoch time\"])\n",
        "\n",
        "with open(filename_save_approx, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"lr\", \"epoch\", \"auc roc\", \"auc roc std\", \"auc prc\", \"auc prc std\", \"val acc\", \"val acc std\", \"batch size\" ,\"epoch time\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ7lAnLkWGXD"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPI-vu-xjwsk"
      },
      "source": [
        "for lr in lr_arr:\n",
        "    !mkdir results/{dataset}/{edge_type}/{time_type}/lstm/lr_{lr}\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    path = 'results/'+dataset+'/'+edge_type+'/'+time_type+'/lstm/lr_'+str(lr)+'/'\n",
        "    \n",
        "    print(\"\\nEpoch no: \", epoch, \"\\tLearning rate: \", lr)\n",
        "    fold = 0\n",
        "    for train, test in kfold.split(X, Y):\n",
        "        fold = fold + 1\n",
        "        filename = path+dataset+'_'+str(fold)+'.csv'\n",
        "        \n",
        "        print('\\nFold no: ', fold)\n",
        "        X_train = X[train]\n",
        "        y_train = Y[train]\n",
        "        X_test = X[test]\n",
        "        y_test = Y[test]\n",
        "        print(y_train)\n",
        "        \n",
        "        model = Sequential()\n",
        "        model.add(Embedding(TOP_WORDS, embedding_vector_length,\n",
        "                            input_length=max_review_length))\n",
        "        model.add(LSTM(50))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                        optimizer=opt, metrics=['accuracy'])\n",
        "        print(model.summary())\n",
        "\n",
        "        model.fit(X_train, y_train, \n",
        "            validation_data=(X_test, y_test), epochs=epoch, batch_size=batch_size, callbacks=[PredictionCallback((X_test, y_test), filename)])\n",
        "\n",
        "    params = {\"kfold\":fold,\"embedding_size\": embedding_vector_length, \"vocab_size\": len(data), \"max_len\": max_len, \"TOP_WORDS\": TOP_WORDS, \"Epoch\": epoch, \"lr\": lr, \"batch_size\": batch_size, \"Positive\": len(reviews_positive), \"Negative\": len(reviews_negative)}\n",
        "    print(params)\n",
        "\n",
        "    with open ('results/'+dataset+'/'+edge_type+'/'+time_type+'/lstm/lr_'+str(lr)+'/params.txt', 'w') as f:\n",
        "        f.write(json.dumps(params, indent=2))\n",
        "    \n",
        "    #save averages\n",
        "    save_avg(path)\n",
        "    \n",
        "    #save top values\n",
        "    top_values(directory, lr, filename_save, filename_save_approx)\n",
        "    \n",
        "print('kfold:', fold)\n",
        "print(dataset)\n",
        "print(\"Top words: \",TOP_WORDS)\n",
        "print(\"Embedding size: \", embedding_vector_length)\n",
        "print('lstm_with_timestamp')\n",
        "print('positive',len(reviews_positive))\n",
        "print('negative',len(reviews_negative))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjPxNSK0AqXA"
      },
      "source": [
        "### Code for GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY8_gIjeXRmc"
      },
      "source": [
        "#### Make directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxdNLIcZAspM"
      },
      "source": [
        "!mkdir results/{dataset}/{edge_type}/{time_type}/gru"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V_coy8KXoC5"
      },
      "source": [
        "#### Hyperparam setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5VnlZa2kER"
      },
      "source": [
        "embedding_vector_length = 32\n",
        "epoch = 350\n",
        "batch_size = 64\n",
        "lr_arr = [0.01, 0.001, 0.0001]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f5itCh4s-Je"
      },
      "source": [
        "#### Setting to save top values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q75u3uWWs-Je"
      },
      "source": [
        "directory = 'results/'+dataset+'/'+edge_type+'/'+time_type+'/gru/'\n",
        "\n",
        "filename_save = directory + \"top_code.csv\"\n",
        "filename_save_approx = directory + \"top_code_rounded.csv\"\n",
        "\n",
        "with open(filename_save, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"lr\", \"epoch\", \"auc roc\", \"auc roc std\", \"auc prc\", \"auc prc std\", \"val acc\", \"val acc std\", \"batch size\" ,\"epoch time\"])\n",
        "\n",
        "with open(filename_save_approx, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"lr\", \"epoch\", \"auc roc\", \"auc roc std\", \"auc prc\", \"auc prc std\", \"val acc\", \"val acc std\", \"batch size\" ,\"epoch time\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaTHa9woX3uF"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYJYviRxBTx2"
      },
      "source": [
        "for lr in lr_arr:\n",
        "    !mkdir results/{dataset}/{edge_type}/{time_type}/gru/lr_{lr}\n",
        "    random.seed(7)\n",
        "    np.random.seed(7)\n",
        "    tf.random.set_seed(7)\n",
        "    path = 'results/'+dataset+'/'+edge_type+'/'+time_type+'/gru/lr_'+str(lr)+'/'\n",
        "    \n",
        "    print(\"\\nEpoch no: \", epoch, \"\\tLearning rate: \", lr)\n",
        "    fold = 0\n",
        "    for train, test in kfold.split(X, Y):\n",
        "        fold = fold + 1\n",
        "        filename = path+dataset+'_'+str(fold)+'.csv'\n",
        "\n",
        "        print('\\nFold no: ', fold)\n",
        "        X_train = X[train]\n",
        "        y_train = Y[train]\n",
        "        X_test = X[test]\n",
        "        y_test = Y[test]\n",
        "        print(len(X_test))\n",
        "        \n",
        "        model = Sequential()\n",
        "        model.add(Embedding(TOP_WORDS, embedding_vector_length,\n",
        "                            input_length=max_review_length))\n",
        "        model.add(GRU(50))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                        optimizer=opt, metrics=['accuracy'])\n",
        "        # print(model.summary())\n",
        "        valx = (X_test, y_test)\n",
        "\n",
        "        model.fit(X_train, y_train, \n",
        "            validation_data=(X_test, y_test), epochs=epoch, batch_size=batch_size, callbacks=[PredictionCallback(valx, filename)])\n",
        "        \n",
        "    params = {\"kfold\":fold,\"embedding_size\": embedding_vector_length, \"vocab_size\": len(data), \"max_len\": max_len, \"TOP_WORDS\": TOP_WORDS, \"Epoch\": epoch, \"lr\": lr, \"batch_size\": batch_size, \"Positive\": len(reviews_positive), \"Negative\": len(reviews_negative)}\n",
        "    print(params)\n",
        "\n",
        "    with open ('results/'+dataset+'/'+edge_type+'/'+time_type+'/gru/lr_'+str(lr)+'/params.txt', 'w') as f:\n",
        "        f.write(json.dumps(params, indent=2))\n",
        "    \n",
        "    #save averages\n",
        "    save_avg(path)\n",
        "    \n",
        "    #save top values\n",
        "    top_values(directory, lr, filename_save, filename_save_approx)\n",
        "    \n",
        "print('kfold:', fold)\n",
        "print(dataset)\n",
        "print(\"Top words: \",TOP_WORDS)\n",
        "print(\"Embedding size: \", embedding_vector_length)\n",
        "print('gru_with_timestamp')\n",
        "print('positive',len(reviews_positive))\n",
        "print('negative',len(reviews_negative))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgjjl3rqCDKR"
      },
      "source": [
        "### Code for BiLSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQS6ACwOXYoR"
      },
      "source": [
        "#### Make directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u8x6-N1CJZu"
      },
      "source": [
        "!mkdir results/{dataset}/{edge_type}/{time_type}/bilstm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieopRXdRXqMV"
      },
      "source": [
        "#### Hyperparam setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp0qg-ZJ-2lm"
      },
      "source": [
        "embedding_vector_length = 32\n",
        "epoch = 350\n",
        "batch_size = 64\n",
        "lr_arr = [0.01, 0.001, 0.0001]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrj2fj03s-Jg"
      },
      "source": [
        "#### Setting to save top values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1O23Btus-Jg"
      },
      "source": [
        "directory = 'results/'+dataset+'/'+edge_type+'/'+time_type+'/bilstm/'\n",
        "\n",
        "filename_save = directory + \"top_code.csv\"\n",
        "filename_save_approx = directory + \"top_code_rounded.csv\"\n",
        "\n",
        "with open(filename_save, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"lr\", \"epoch\", \"auc roc\", \"auc roc std\", \"auc prc\", \"auc prc std\", \"val acc\", \"val acc std\", \"batch size\" ,\"epoch time\"])\n",
        "\n",
        "with open(filename_save_approx, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"lr\", \"epoch\", \"auc roc\", \"auc roc std\", \"auc prc\", \"auc prc std\", \"val acc\", \"val acc std\", \"batch size\" ,\"epoch time\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkstsdK-Kbky"
      },
      "source": [
        "print('positive',len(reviews_positive))\n",
        "print('negative',len(reviews_negative))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNNdXNOqX6lD"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xjvXiUgCV9_"
      },
      "source": [
        "for lr in lr_arr:\n",
        "    !mkdir results/{dataset}/{edge_type}/{time_type}/bilstm/lr_{lr}\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    path = 'results/'+dataset+'/'+edge_type+'/'+time_type+'/bilstm/lr_'+str(lr)+'/'\n",
        "    \n",
        "    print(\"\\nEpoch no: \", epoch, \"\\tLearning rate: \", lr)\n",
        "    fold = 0\n",
        "    for train, test in kfold.split(X, Y):\n",
        "        fold = fold+1\n",
        "        filename = path+dataset+'_'+str(fold)+'.csv'\n",
        "       \n",
        "        print('\\nFold no: ', fold)\n",
        "        X_train = X[train]\n",
        "        y_train = Y[train]\n",
        "        X_test = X[test]\n",
        "        y_test = Y[test]\n",
        "        \n",
        "        model = Sequential()\n",
        "        model.add(Embedding(TOP_WORDS, embedding_vector_length,\n",
        "                            input_length=max_review_length))\n",
        "        model.add(Bidirectional(LSTM(50)))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                        optimizer=opt, metrics=['accuracy'])\n",
        "        print(model.summary())\n",
        "\n",
        "        model.fit(X_train, y_train, \n",
        "            validation_data=(X_test, y_test), epochs=epoch, batch_size=batch_size, callbacks=[PredictionCallback((X_test, y_test), filename)])\n",
        "\n",
        "    params = {\"kfold\":fold,\"embedding_size\": embedding_vector_length, \"vocab_size\": len(data), \"max_len\": max_len, \"TOP_WORDS\": TOP_WORDS, \"Epoch\": epoch, \"lr\": lr, \"batch_size\": batch_size, \"Positive\": len(reviews_positive), \"Negative\": len(reviews_negative)}\n",
        "    print(params)\n",
        "\n",
        "    with open ('esults/'+dataset+'/'+edge_type+'/'+time_type+'/bilstm/lr_'+str(lr)+'/params.txt', 'w') as f:\n",
        "        f.write(json.dumps(params, indent=2))\n",
        "\n",
        "    #save averages\n",
        "    save_avg(path)\n",
        "    \n",
        "    #save top values\n",
        "    top_values(directory, lr, filename_save, filename_save_approx)\n",
        "    \n",
        "print('kfold:', fold)\n",
        "print(dataset)\n",
        "print(\"Top words: \",TOP_WORDS)\n",
        "print(\"Embedding size: \", embedding_vector_length)\n",
        "print('bilstm_without_timestamp')\n",
        "print('positive',len(reviews_positive))\n",
        "print('negative',len(reviews_negative))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ShVr9og0jvB"
      },
      "source": [
        "### Code for Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl66t6dVXa6z"
      },
      "source": [
        "#### Make directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLy5anab0nKh"
      },
      "source": [
        "!mkdir results/{dataset}/{edge_type}/{time_type}/transformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRcE0IkzXsEB"
      },
      "source": [
        "#### Hyperparam setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO4dZh7pLqPI"
      },
      "source": [
        "epoch = 350\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "batch_size = 64\n",
        "lr_arr = [0.01, 0.001, 0.0001]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoL7LalBs-Ji"
      },
      "source": [
        "#### Setting to save top values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3uDNUK1s-Ji"
      },
      "source": [
        "directory = 'results/'+dataset+'/'+edge_type+'/'+time_type+'/transformer/'\n",
        "\n",
        "filename_save = directory + \"top_code.csv\"\n",
        "filename_save_approx = directory + \"top_code_rounded.csv\"\n",
        "\n",
        "with open(filename_save, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"lr\", \"epoch\", \"auc roc\", \"auc roc std\", \"auc prc\", \"auc prc std\", \"val acc\", \"val acc std\", \"batch size\" ,\"epoch time\"])\n",
        "\n",
        "with open(filename_save_approx, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"lr\", \"epoch\", \"auc roc\", \"auc roc std\", \"auc prc\", \"auc prc std\", \"val acc\", \"val acc std\", \"batch size\" ,\"epoch time\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrrmwhj_X9fu"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9gaAM_i0njU"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EABnxqyc1ZVg"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnOMOoBy1ZN0"
      },
      "source": [
        "for lr in lr_arr:\n",
        "    !mkdir results/{dataset}/{edge_type}/{time_type}/transformer/lr_{lr}\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    path = 'results/'+dataset+'/'+edge_type+'/'+time_type+'/transformer/lr_'+str(lr)+'/'\n",
        "\n",
        "    print(\"\\nEpoch no: \", epoch, \"\\tLearning rate: \", lr)\n",
        "    fold = 0\n",
        "    for train, test in kfold.split(X, Y):\n",
        "        fold = fold+1\n",
        "        filename = path+dataset+'_'+str(fold)+'.csv'\n",
        "        print('\\nFold no: ', fold)\n",
        "        X_train = X[train]\n",
        "        y_train = Y[train]\n",
        "        X_test = X[test]\n",
        "        y_test = Y[test]\n",
        "        # print(len(X_test))\n",
        "\n",
        "        inputs = layers.Input(shape=(max_len,))\n",
        "        embedding_layer = TokenAndPositionEmbedding(max_len, TOP_WORDS, embed_dim)\n",
        "        x = embedding_layer(inputs)\n",
        "        print(x)\n",
        "        transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "        x = transformer_block(x)\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "        x = layers.Dropout(0.1)(x)\n",
        "        x = layers.Dense(20, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(0.1)(x)\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                        optimizer=opt, metrics=['accuracy'])\n",
        "        # print(model.summary())\n",
        "        valx = (X_test, y_test)\n",
        "\n",
        "        model.fit(X_train, y_train, \n",
        "            validation_data=(X_test, y_test), epochs=epoch, batch_size=batch_size, callbacks=[PredictionCallback(valx, filename)])\n",
        "\n",
        "    params = {\"kfold\":fold, \"embedding_size\": embed_dim, \"num_heads\": num_heads, \"ff_dim\": ff_dim, \"vocab_size\": len(data), \"max_len\": max_len, \"TOP_WORDS\": TOP_WORDS, \"Epoch\": epoch, \"lr\": lr_arr, \"batch_size\": batch_size, \"Positive\": len(reviews_positive), \"Negative\": len(reviews_negative)}\n",
        "    print(params)\n",
        "\n",
        "    with open ('results/'+dataset+'/'+edge_type+'/'+time_type+'/transformer/lr_'+str(lr)+'/params.txt', 'w') as f:\n",
        "        f.write(json.dumps(params, indent=2))\n",
        "        \n",
        "    #save averages\n",
        "    save_avg(path)\n",
        "    \n",
        "    #save top values\n",
        "    top_values(directory, lr, filename_save, filename_save_approx)\n",
        "\n",
        "print('kfold:', fold)\n",
        "print(dataset)\n",
        "print(\"Top words: \",TOP_WORDS)\n",
        "print(\"Embedding size: \", embed_dim)\n",
        "print('transformer_without_timestamp')\n",
        "print('positive',len(reviews_positive))\n",
        "print('negative',len(reviews_negative))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}