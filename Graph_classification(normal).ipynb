{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph classification(normal).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4u1lj3MTNSLA",
        "19r0U7iK9YhQ",
        "Qh71qPti9PmG",
        "UsxEI-5zZPyi",
        "D8sKYu9UZYhm",
        "rGoYu9FQqwaG",
        "-oDyiiCwU36K",
        "wtUNI9-50o4V"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL38gv1n8OLW"
      },
      "source": [
        "###Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfFQCH0-1Xxv"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbTf3yy--qbQ"
      },
      "source": [
        "%cd /content/drive/MyDrive/Graph_classification/\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u1lj3MTNSLA"
      },
      "source": [
        "###Code for create graph and sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvq1feD3LEQ2"
      },
      "source": [
        "!bash build.sh"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyPde5VA_Nhk"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import pickle\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import json\n",
        "from args import Args\n",
        "from utils import create_dirs\n",
        "from datasets.process_dataset import create_graphs\n",
        "from datasets.process_sequence import create_sequences\n",
        "from datasets.preprocess import calc_max_prev_node, dfscodes_weights\n",
        "seed = 7\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = Args()\n",
        "    args = args.update_args()\n",
        "\n",
        "    create_dirs(args)\n",
        "    \n",
        "    random.seed(seed)\n",
        "\n",
        "    #Create graphs and its min dfs code\n",
        "    graphs, create_graphs_time, dfscode_time = create_graphs(args)\n",
        "   \n",
        "\n",
        "    # show graphs statistics\n",
        "    # print('Model:', args.note)\n",
        "    print('Device:', args.device)\n",
        "    print('Graph type:', args.graph_type)\n",
        "\n",
        "    # Loading the feature map\n",
        "    with open(args.current_dataset_path + 'label_0/map.dict', 'rb') as f:\n",
        "        feature_map = pickle.load(f)\n",
        "    print('\\n\\nInformation about Graphs of class 0: \\n')\n",
        "    print('Max number of nodes: {}'.format(feature_map['max_nodes']))\n",
        "    print('Max number of edges: {}'.format(feature_map['max_edges']))\n",
        "    print('Min number of nodes: {}'.format(feature_map['min_nodes']))\n",
        "    print('Min number of edges: {}'.format(feature_map['min_edges']))\n",
        "    print('Max degree of a node: {}'.format(feature_map['max_degree']))\n",
        "    print('No. of node labels: {}'.format(len(feature_map['node_forward'])))\n",
        "    print('No. of edge labels: {}'.format(len(feature_map['edge_forward'])))\n",
        "\n",
        "\n",
        "    with open(args.current_dataset_path + 'label_1/map.dict', 'rb') as f:\n",
        "        feature_map = pickle.load(f)\n",
        "    print('\\n\\nInformation about Graphs of class 1: \\n')\n",
        "    print('Max number of nodes: {}'.format(feature_map['max_nodes']))\n",
        "    print('Max number of edges: {}'.format(feature_map['max_edges']))\n",
        "    print('Min number of nodes: {}'.format(feature_map['min_nodes']))\n",
        "    print('Min number of edges: {}'.format(feature_map['min_edges']))\n",
        "    print('Max degree of a node: {}'.format(feature_map['max_degree']))\n",
        "    print('No. of node labels: {}'.format(len(feature_map['node_forward'])))\n",
        "    print('No. of edge labels: {}'.format(len(feature_map['edge_forward'])))\n",
        "\n",
        "    with open(args.current_dataset_path + 'all_graphs/map.dict', 'rb') as f:\n",
        "        feature_map = pickle.load(f)\n",
        "    print('\\n\\nInformation about All the Graphs: \\n')\n",
        "    print('Max number of nodes: {}'.format(feature_map['max_nodes']))\n",
        "    print('Max number of edges: {}'.format(feature_map['max_edges']))\n",
        "    print('Min number of nodes: {}'.format(feature_map['min_nodes']))\n",
        "    print('Min number of edges: {}'.format(feature_map['min_edges']))\n",
        "    print('Max degree of a node: {}'.format(feature_map['max_degree']))\n",
        "    print('No. of node labels: {}'.format(len(feature_map['node_forward'])))\n",
        "    print('No. of edge labels: {}'.format(len(feature_map['edge_forward'])))\n",
        "\n",
        "    #Create equences of min dfs code\n",
        "    create_sequences_time = create_sequences(args)\n",
        "\n",
        "    param_time = {\"create_graphs\" : create_graphs_time,\n",
        "                  \"dfscode\": dfscode_time,\n",
        "                  \"create_sequence\": create_sequences_time}\n",
        "                  \n",
        "    with open ('/content/drive/MyDrive/Graph_classification/datasets/'+args.graph_type+'/time.txt', 'w') as f:\n",
        "        f.write(json.dumps(param_time, indent=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19r0U7iK9YhQ"
      },
      "source": [
        "###Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aczq8Uzljydk"
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# from bow import Vocabulary\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Embedding, LSTM, GRU, Bidirectional\n",
        "import keras\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "import random\n",
        "import csv\n",
        "seed = 7\n",
        "\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LGfVB7FqodM"
      },
      "source": [
        "###Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7XJxz8DFNng"
      },
      "source": [
        "####Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHiVQCUfpaox"
      },
      "source": [
        "!mkdir results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7YRzLlIcMOt"
      },
      "source": [
        "dataset = 'DBLP_v1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIkGNVdbwuUt"
      },
      "source": [
        "!mkdir results/{dataset}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PWW4Ih0VxSW"
      },
      "source": [
        "!mkdir results/{dataset}/normal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2njKhjAkZ10-"
      },
      "source": [
        "time_type = 'w'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lybrFztDDlsa"
      },
      "source": [
        "!mkdir results/{dataset}/normal/{time_type}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh71qPti9PmG"
      },
      "source": [
        "####Function to create bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkU3RJ3Y80oR"
      },
      "source": [
        "from collections import Counter\n",
        "import json\n",
        "import numpy as np\n",
        " \n",
        "class Vocabulary:\n",
        " \n",
        "    def __init__(self, vocabulary, wordFrequencyFilePath):\n",
        "        self.vocabulary = vocabulary\n",
        "        self.WORD_FREQUENCY_FILE_FULL_PATH = wordFrequencyFilePath\n",
        "        self.input_word_index = {}\n",
        "        self.reverse_input_word_index = {}\n",
        " \n",
        "        self.MaxSentenceLength = None\n",
        " \n",
        "    def PrepareVocabulary(self, reviews):\n",
        "        self._prepare_Word_Frequency_Count_File(reviews)\n",
        "        self._create_Vocab_Indexes()\n",
        " \n",
        "        self.MaxSentenceLength = max([len(txt.split(\" \")) for txt in reviews])\n",
        " \n",
        "    def Get_Top_Words(self, number_words=None):\n",
        "        if number_words == None:\n",
        "            number_words = self.vocabulary\n",
        " \n",
        "        chars = json.loads(open(self.WORD_FREQUENCY_FILE_FULL_PATH).read())\n",
        "        counter = Counter(chars)\n",
        "        most_popular_words = {key for key,\n",
        "                              _value in counter.most_common(number_words)}\n",
        "        return most_popular_words\n",
        " \n",
        "    def _prepare_Word_Frequency_Count_File(self, reviews):\n",
        "        counter = Counter()\n",
        "        for s in reviews:\n",
        "            counter.update(s.split(\" \"))\n",
        " \n",
        "        with open(self.WORD_FREQUENCY_FILE_FULL_PATH, 'w') as output_file:\n",
        "            output_file.write(json.dumps(counter))\n",
        " \n",
        "    def _create_Vocab_Indexes(self):\n",
        "        INPUT_WORDS = self.Get_Top_Words(self.vocabulary)\n",
        " \n",
        "        for i, word in enumerate(INPUT_WORDS):\n",
        "            self.input_word_index[word] = i\n",
        " \n",
        "        for word, i in self.input_word_index.items():\n",
        "            self.reverse_input_word_index[i] = word\n",
        " \n",
        "    def TransformSentencesToId(self, sentences):\n",
        "        vectors = []\n",
        "        for r in sentences:\n",
        "            words = r.split(\" \")\n",
        "            vector = np.zeros(len(words))\n",
        " \n",
        "            for t, word in enumerate(words):\n",
        "                if word in self.input_word_index:\n",
        "                    vector[t] = self.input_word_index[word]\n",
        "                else:\n",
        "                    pass\n",
        " \n",
        "            vectors.append(vector)\n",
        " \n",
        "        return vectors\n",
        " \n",
        "    def ReverseTransformSentencesToId(self, sentences):\n",
        "        vectors = []\n",
        "        for r in sentences:\n",
        "            words = r.split(\" \")\n",
        "            vector = np.zeros(len(words))\n",
        " \n",
        "            for t, word in enumerate(words):\n",
        "                if word in self.input_word_index:\n",
        "                    vector[t] = self.input_word_index[word]\n",
        "                else:\n",
        "                    pass\n",
        "                    # vector[t] = 2 #unk\n",
        "            vectors.append(vector)\n",
        " \n",
        "        return vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsxEI-5zZPyi"
      },
      "source": [
        "####with timestamp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A2iWScJqu-U"
      },
      "source": [
        "if time_type == 'w':\n",
        "  path = '/content/drive/MyDrive/Graph_classification/datasets/'+dataset+'/sequences/with_timestamp/'\n",
        "\n",
        "  with open(path + 'label_1/with_timestamp_sequence_label_1.txt', 'r') as f:\n",
        "      reviews_positive = json.loads(f.read())\n",
        "\n",
        "  with open(path + 'label_0/with_timestamp_sequence_label_0.txt', 'r') as f:\n",
        "      reviews_negative = json.loads(f.read())\n",
        "\n",
        "  # print(reviews_negative)\n",
        "  if dataset == 'MUTAG':\n",
        "      TOP_WORDS = 372\n",
        "  elif dataset == 'PTC_FR':\n",
        "      TOP_WORDS = 1253\n",
        "  elif dataset == 'NCI-H23':\n",
        "      TOP_WORDS = 7260\n",
        "  elif dataset == 'TOX21_AR':\n",
        "      TOP_WORDS = 8780\n",
        "  elif dataset == 'DBLP_v1':\n",
        "      TOP_WORDS = 58184\n",
        "  else:\n",
        "      TOP_WORDS = 100000\n",
        "\n",
        "  print(dataset)\n",
        "  print(\"TOP WORDS: \",TOP_WORDS)\n",
        "\n",
        "  Reviews_Labeled = list(zip(reviews_positive, np.ones(len(reviews_positive))))\n",
        "  Reviews_Labeled.extend(\n",
        "      list(zip(reviews_negative, np.zeros(len(reviews_negative)))))\n",
        "  random.seed(7)\n",
        "  random.shuffle(Reviews_Labeled)\n",
        "  # print(Reviews_Labeled)\n",
        "  print(Reviews_Labeled)\n",
        "  vocab = Vocabulary(TOP_WORDS, path + \"analysis.vocab\")\n",
        "\n",
        "  reviews_text = [line[0] for line in Reviews_Labeled]\n",
        "  vocab.PrepareVocabulary(reviews_text)\n",
        "\n",
        "  with open(path + \"analysis.vocab\") as f:\n",
        "      data = json.load(f)\n",
        "      print('Total number of words: ' + str(len(data)))\n",
        "\n",
        "  reviews, labels = zip(*Reviews_Labeled)\n",
        "  reviews_int = vocab.TransformSentencesToId(reviews)\n",
        "\n",
        "  X = np.array(reviews_int, dtype=object)\n",
        "  max_len = max([len(i) for i in X])\n",
        "  print(\"Maximum length :\", max_len)\n",
        "  max_review_length = max_len\n",
        "  X = sequence.pad_sequences(X, maxlen=max_review_length)\n",
        "  Y = np.array(labels)\n",
        "  # epoch_arr = [25, 50, 100, 150, 200, 250]\n",
        "  lr_arr = [0.01, 0.001, 0.0001]\n",
        "  kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0FSHyd2WVdC"
      },
      "source": [
        "if time_type == 'w': \n",
        "  # print(kfold)\n",
        "  fold_index = {\"train\": [], \"test\": []}\n",
        "  for train_index, test_index in kfold.split(X,Y):\n",
        "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = Y[train_index], Y[test_index]\n",
        "    fold_index[\"train\"].append(train_index.tolist())\n",
        "    fold_index[\"test\"].append(test_index.tolist())\n",
        "\n",
        "  print(fold_index['test'])\n",
        "  with open ('/content/drive/MyDrive/Graph_classification/results/'+dataset+'/normal/w/indexes.txt', 'w') as f:\n",
        "      f.write(json.dumps(fold_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8sKYu9UZYhm"
      },
      "source": [
        "####without timestamp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfk35beLWz7u"
      },
      "source": [
        "if time_type == 'wo':\n",
        "  path = '/content/drive/MyDrive/Graph_classification/datasets/'+dataset+'/sequences/without_timestamp/'\n",
        "\n",
        "  with open(path + 'label_1/without_timestamp_sequence_label_1.txt', 'r') as f:\n",
        "      reviews_positive = json.loads(f.read())\n",
        "      # print(reviews_positive)\n",
        "  with open(path + 'label_0/without_timestamp_sequence_label_0.txt', 'r') as f:\n",
        "      reviews_negative = json.loads(f.read())\n",
        "\n",
        "  # print(reviews_negative)\n",
        "  if dataset == 'MUTAG':\n",
        "      TOP_WORDS = 23\n",
        "  elif dataset == 'PTC_FR' or dataset == 'PTC_FR_ISO':\n",
        "      TOP_WORDS = 70\n",
        "  elif dataset == 'NCI-H23':\n",
        "      TOP_WORDS = 80\n",
        "  elif dataset == 'TOX21_AR':\n",
        "      TOP_WORDS = 178\n",
        "  elif dataset == 'DBLP_v1':\n",
        "      TOP_WORDS = 47295\n",
        "  else :\n",
        "      TOP_WORDS = 200\n",
        "\n",
        "  print(dataset)\n",
        "  print(\"TOP WORDS: \",TOP_WORDS)\n",
        "\n",
        "  Reviews_Labeled = list(zip(reviews_positive, np.ones(len(reviews_positive))))\n",
        "  Reviews_Labeled.extend(\n",
        "      list(zip(reviews_negative, np.zeros(len(reviews_negative)))))\n",
        "  random.seed(7)\n",
        "  random.shuffle(Reviews_Labeled)\n",
        "  print(Reviews_Labeled)\n",
        "  vocab = Vocabulary(TOP_WORDS, path + \"analysis.vocab\")\n",
        "\n",
        "  reviews_text = [line[0] for line in Reviews_Labeled]\n",
        "  vocab.PrepareVocabulary(reviews_text)\n",
        "\n",
        "  with open(path + \"analysis.vocab\") as f:\n",
        "      data = json.load(f)\n",
        "      print('Total number of words: ' + str(len(data)))\n",
        "\n",
        "  reviews, labels = zip(*Reviews_Labeled)\n",
        "  reviews_int = vocab.TransformSentencesToId(reviews)\n",
        "\n",
        "  X = np.array(reviews_int, dtype=object)\n",
        "  max_len = max([len(i) for i in X])\n",
        "  print(\"Maximum length :\", max_len)\n",
        "  max_review_length = max_len\n",
        "  X = sequence.pad_sequences(X, maxlen=max_review_length)\n",
        "  Y = np.array(labels)\n",
        "  # epoch_arr = [25, 50, 100, 150, 200, 250]\n",
        "  lr_arr = [0.01, 0.001, 0.0001]\n",
        "  kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIUYnM86XU1w"
      },
      "source": [
        "if time_type == 'wo':\n",
        "  fold_index = {\"train\": [], \"test\": []}\n",
        "  for train_index, test_index in kfold.split(X,Y):\n",
        "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = Y[train_index], Y[test_index]\n",
        "    fold_index[\"train\"].append(train_index.tolist())\n",
        "    fold_index[\"test\"].append(test_index.tolist())\n",
        "\n",
        "  print(fold_index['test'])\n",
        "  with open ('/content/drive/MyDrive/Graph_classification/results/'+dataset+'/normal/wo/indexes.txt', 'w') as f:\n",
        "      f.write(json.dumps(fold_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UojlQv-TcqKT"
      },
      "source": [
        "with open ('/content/drive/MyDrive/Graph_classification/results/'+dataset+'/normal/wo/indexes.txt', 'w') as f:\n",
        "    f.write(json.dumps(fold_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGoYu9FQqwaG"
      },
      "source": [
        "###CallBack function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6k2vcapj4cQ"
      },
      "source": [
        "\n",
        "# define your custom callback for prediction\n",
        "class PredictionCallback(tf.keras.callbacks.Callback): \n",
        "    def __init__(self, valx, file):\n",
        "        super().__init__()\n",
        "        self.validation_data = valx\n",
        "        self.filename = file\n",
        "        # self.batch_size = batch_size\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        with open(filename, 'w', newline='') as file:\n",
        "          writer = csv.writer(file)\n",
        "          writer.writerow([\"roc\", \"prc\", \"train loss\", \"train acc\", \"val loss\", \"val acc\", \"epoch time\"])\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # print(\"yes\")\n",
        "        # print(self.validation_data)\n",
        "        # print(\"yo\")\n",
        "        # print(logs)\n",
        "        self.epoch_time_end = time.time() - self.epoch_time_start\n",
        "        yhat_probs = self.model.predict(self.validation_data[0])\n",
        "        # print('prediction: {} at epoch: {}'.format(yhat_probs, epoch))\n",
        "\n",
        "            # yhat_probs = model.predict(X_test, verbose=0)\n",
        "            # predict crisp classes for test set\n",
        "            # yhat_classes = model.predict_classes(X_test, verbose=0)\n",
        "            # reduce to 1d array\n",
        "        yhat_probs = yhat_probs[:, 0]\n",
        "            # yhat_classes = yhat_classes[:, 0]\n",
        "\n",
        "        roc_auc = metrics.roc_auc_score(self.validation_data[1], yhat_probs, average='macro')\n",
        "        print('ROC AUC: %f' % roc_auc)\n",
        "        # cvscoresroc.append(roc_auc)\n",
        "\n",
        "        prc_auc = metrics.average_precision_score(\n",
        "            self.validation_data[1], yhat_probs, average='macro', pos_label=1)\n",
        "        print('PRC AUC: %f' % prc_auc)\n",
        "        # cvscoresprc.append(prc_auc)\n",
        "\n",
        "        # Final evaluation of the models\n",
        "        scores = model.evaluate(self.validation_data[0], self.validation_data[1], verbose=0)\n",
        "        print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "        # cvscores.append(scores[1] * 100)\n",
        "        # print(len(reviews_positive))\n",
        "        # print(len(reviews_negative))\n",
        "        with open(self.filename, 'a') as f_object:\n",
        "        # Pass this file object to csv.writer()\n",
        "        # and get a writer object\n",
        "            writer_object = csv.writer(f_object)\n",
        "            # Pass the list as an argument into\n",
        "            # the writerow()\n",
        "            # writer_object.writerow([roc_auc, prc_auc, scores[0], scores[1]*100])\n",
        "            writer_object.writerow([\"%.4f\" % roc_auc, \"%.4f\" % prc_auc,\"%.4f\" % logs['loss'], \"%.4f\" % (logs['accuracy']*100), \"%.4f\" % scores[0], \"%.4f\" % (scores[1]*100), \"%.5f\" % self.epoch_time_end])\n",
        "\n",
        "            # \"%.2f\" % a_number\n",
        "            #Close the file object\n",
        "            f_object.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_gFcZvO0b9q"
      },
      "source": [
        "###Function to save averages and its standard deviation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuPSPsv70fHa"
      },
      "source": [
        "def save_avg(input_path):\n",
        "  avg_in = list()\n",
        "  directory = input_path\n",
        "  for filename in os.listdir(directory):\n",
        "          if filename.endswith(\".csv\") and filename != \"average.csv\" and filename != \"average_with_std.csv\":\n",
        "                  print(\"file: \",filename)\n",
        "                  df = pd.read_csv(directory+filename)\n",
        "                  print(df.to_numpy()[0][0])\n",
        "                  avg_in.append(df.to_numpy().tolist())\n",
        "\n",
        "  avg = np.mean(avg_in,axis=0)\n",
        "  avg_std = np.std(avg_in,axis=0)\n",
        "  # print(avg_std[:,5])\n",
        "  # avg = np.mean(avg,axis=0)\n",
        "  # print(avg)\n",
        "  save_file = directory + \"average_with_std.csv\"\n",
        "  header = [\"roc\", \"prc\",  \"train loss\", \"train acc\", \"val loss\", \"val acc\", \"epoch time\"]\n",
        "  header_std = [\"roc std\", \"prc std\", \"val acc std\"]\n",
        "  data = pd.DataFrame({header[0]: avg[:,0], header_std[0]:avg_std[:,0], header[1]: avg[:,1], header_std[1]:avg_std[:,1], header[2]: avg[:,2], header[3]: avg[:,3], header[4]: avg[:,4], header[5]: avg[:,5], header_std[2]:avg_std[:,5], header[6]: avg[:,6]})\n",
        "  data.to_csv(save_file, index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBmQ5lR97cOI"
      },
      "source": [
        "###Code for LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMpV7Cc_V505"
      },
      "source": [
        "####Make directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyBAXmlS-LpY"
      },
      "source": [
        "!mkdir results/{dataset}/normal/{time_type}/lstm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aby6ombjXT2"
      },
      "source": [
        "print('positive',len(reviews_positive))\n",
        "print('negative',len(reviews_negative))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyuYkIrwVu6l"
      },
      "source": [
        "####Hyperparam setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aemVKjw75S4W"
      },
      "source": [
        "embedding_vector_length = 32\n",
        "epoch = 350\n",
        "batch_size = 64\n",
        "lr_arr = [0.01, 0.001, 0.0001]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ7lAnLkWGXD"
      },
      "source": [
        "####Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPI-vu-xjwsk"
      },
      "source": [
        "for lr in lr_arr:\n",
        "    !mkdir results/{dataset}/normal/{time_type}/lstm/lr_{lr}\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    path = '/content/drive/MyDrive/Graph_classification/results/'+dataset+'/normal/'+time_type+'/lstm/lr_'+str(lr)+'/'\n",
        "    \n",
        "    print(\"\\nEpoch no: \", epoch, \"\\tLearning rate: \", lr)\n",
        "    fold = 0\n",
        "    for train, test in kfold.split(X, Y):\n",
        "        fold = fold + 1\n",
        "        filename = path+dataset+'_'+str(fold)+'.csv'\n",
        "        \n",
        "        print('\\nFold no: ', fold)\n",
        "        X_train = X[train]\n",
        "        y_train = Y[train]\n",
        "        X_test = X[test]\n",
        "        y_test = Y[test]\n",
        "        print(y_train)\n",
        "        \n",
        "        model = Sequential()\n",
        "        model.add(Embedding(TOP_WORDS, embedding_vector_length,\n",
        "                            input_length=max_review_length))\n",
        "        model.add(LSTM(50))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        opt = keras.optimizers.Adam(learning_rate=lr)\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                        optimizer=opt, metrics=['accuracy'])\n",
        "        print(model.summary())\n",
        "\n",
        "        model.fit(X_train, y_train, \n",
        "            validation_data=(X_test, y_test), epochs=epoch, batch_size=batch_size, callbacks=[PredictionCallback((X_test, y_test), filename)])\n",
        "\n",
        "    params = {\"kfold\":fold,\"embedding_size\": embedding_vector_length, \"vocab_size\": len(data), \"max_len\": max_len, \"TOP_WORDS\": TOP_WORDS, \"Epoch\": epoch, \"lr\": lr, \"batch_size\": batch_size, \"Positive\": len(reviews_positive), \"Negative\": len(reviews_negative)}\n",
        "    print(params)\n",
        "\n",
        "    with open ('/content/drive/MyDrive/Graph_classification/results/'+dataset+'/normal/'+time_type+'/lstm/lr_'+str(lr)+'/params.txt', 'w') as f:\n",
        "        f.write(json.dumps(params, indent=2))\n",
        "    \n",
        "    save_avg(path)\n",
        "\n",
        "print('kfold:', 5)\n",
        "print(dataset)\n",
        "print(\"Top words: \",TOP_WORDS)\n",
        "print(\"Embedding size: \", embedding_vector_length)\n",
        "print('lstm_with_timestamp')\n",
        "print('positive',len(reviews_positive))\n",
        "print('negative',len(reviews_negative))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjPxNSK0AqXA"
      },
      "source": [
        "###Code for GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY8_gIjeXRmc"
      },
      "source": [
        "####Make directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxdNLIcZAspM"
      },
      "source": [
        "!mkdir results/{dataset}/normal/{time_type}/gru"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V_coy8KXoC5"
      },
      "source": [
        "####Hyperparam setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5VnlZa2kER"
      },
      "source": [
        "embedding_vector_length = 32\n",
        "epoch = 350\n",
        "batch_size = 64\n",
        "lr_arr = [0.01, 0.001, 0.0001]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaTHa9woX3uF"
      },
      "source": [
        "####Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYJYviRxBTx2"
      },
      "source": [
        "for lr in lr_arr:\n",
        "    !mkdir results/{dataset}/normal/{time_type}/gru/lr_{lr}\n",
        "    random.seed(7)\n",
        "    np.random.seed(7)\n",
        "    tf.random.set_seed(7)\n",
        "    path = '/content/drive/MyDrive/Graph_classification/results/'+dataset+'/normal/'+time_type+'/gru/lr_'+str(lr)+'/'\n",
        "    \n",
        "    print(\"\\nEpoch no: \", epoch, \"\\tLearning rate: \", lr)\n",
        "    fold = 0\n",
        "    for train, test in kfold.split(X, Y):\n",
        "        fold = fold + 1\n",
        "        filename = path+dataset+'_'+str(fold)+'.csv'\n",
        "        # with open(filename, 'w', newline='') as file:\n",
        "        #     writer = csv.writer(file)\n",
        "        #     writer.writerow([\"roc\", \"prc\", \"val loss\", \"val acc\"])\n",
        "\n",
        "        print('\\nFold no: ', fold)\n",
        "        X_train = X[train]\n",
        "        y_train = Y[train]\n",
        "        X_test = X[test]\n",
        "        y_test = Y[test]\n",
        "        print(len(X_test))\n",
        "        \n",
        "        model = Sequential()\n",
        "        model.add(Embedding(TOP_WORDS, embedding_vector_length,\n",
        "                            input_length=max_review_length))\n",
        "        model.add(GRU(50))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        opt = keras.optimizers.Adam(learning_rate=lr)\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                        optimizer=opt, metrics=['accuracy'])\n",
        "        # print(model.summary())\n",
        "        valx = (X_test, y_test)\n",
        "\n",
        "        model.fit(X_train, y_train, \n",
        "            validation_data=(X_test, y_test), epochs=epoch, batch_size=batch_size, callbacks=[PredictionCallback(valx, filename)])\n",
        "        \n",
        "    params = {\"kfold\":fold,\"embedding_size\": embedding_vector_length, \"vocab_size\": len(data), \"max_len\": max_len, \"TOP_WORDS\": TOP_WORDS, \"Epoch\": epoch, \"lr\": lr, \"batch_size\": batch_size, \"Positive\": len(reviews_positive), \"Negative\": len(reviews_negative)}\n",
        "    print(params)\n",
        "\n",
        "    with open ('/content/drive/MyDrive/Graph_classification/results/'+dataset+'/normal/'+time_type+'/gru/lr_'+str(lr)+'/params.txt', 'w') as f:\n",
        "        f.write(json.dumps(params, indent=2))\n",
        "\n",
        "print('kfold:', 5)\n",
        "print(dataset)\n",
        "print(\"Top words: \",TOP_WORDS)\n",
        "print(\"Embedding size: \", embedding_vector_length)\n",
        "print('gru_with_timestamp')\n",
        "print('positive',len(reviews_positive))\n",
        "print('negative',len(reviews_negative))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgjjl3rqCDKR"
      },
      "source": [
        "###Code for BiLSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQS6ACwOXYoR"
      },
      "source": [
        "####Make directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u8x6-N1CJZu"
      },
      "source": [
        "!mkdir results/{dataset}/normal/{time_type}/bilstm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieopRXdRXqMV"
      },
      "source": [
        "####Hyperparam setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp0qg-ZJ-2lm"
      },
      "source": [
        "embedding_vector_length = 32\n",
        "epoch = 350\n",
        "batch_size = 64\n",
        "lr_arr = [0.01, 0.001, 0.0001]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkstsdK-Kbky"
      },
      "source": [
        "print('positive',len(reviews_positive))\n",
        "print('negative',len(reviews_negative))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNNdXNOqX6lD"
      },
      "source": [
        "####Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xjvXiUgCV9_"
      },
      "source": [
        "for lr in lr_arr:\n",
        "    !mkdir results/{dataset}/normal/{time_type}/bilstm/lr_{lr}\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    path = '/content/drive/MyDrive/Graph_classification/results/'+dataset+'/normal/'+time_type+'/bilstm/lr_'+str(lr)+'/'\n",
        "    \n",
        "    print(\"\\nEpoch no: \", epoch, \"\\tLearning rate: \", lr)\n",
        "    fold = 0\n",
        "    for train, test in kfold.split(X, Y):\n",
        "        fold = fold+1\n",
        "        filename = path+dataset+'_'+str(fold)+'.csv'\n",
        "       \n",
        "        print('\\nFold no: ', fold)\n",
        "        X_train = X[train]\n",
        "        y_train = Y[train]\n",
        "        X_test = X[test]\n",
        "        y_test = Y[test]\n",
        "        # print(len(X_test))\n",
        "        \n",
        "        model = Sequential()\n",
        "        model.add(Embedding(TOP_WORDS, embedding_vector_length,\n",
        "                            input_length=max_review_length))\n",
        "        model.add(Bidirectional(LSTM(50)))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        opt = keras.optimizers.Adam(learning_rate=lr)\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                        optimizer=opt, metrics=['accuracy'])\n",
        "        print(model.summary())\n",
        "\n",
        "        model.fit(X_train, y_train, \n",
        "            validation_data=(X_test, y_test), epochs=epoch, batch_size=batch_size, callbacks=[PredictionCallback((X_test, y_test), filename)])\n",
        "\n",
        "    params = {\"kfold\":fold,\"embedding_size\": embedding_vector_length, \"vocab_size\": len(data), \"max_len\": max_len, \"TOP_WORDS\": TOP_WORDS, \"Epoch\": epoch, \"lr\": lr, \"batch_size\": batch_size, \"Positive\": len(reviews_positive), \"Negative\": len(reviews_negative)}\n",
        "    print(params)\n",
        "\n",
        "    with open ('/content/drive/MyDrive/Graph_classification/results/'+dataset+'/normal/'+time_type+'/bilstm/lr_'+str(lr)+'/params.txt', 'w') as f:\n",
        "        f.write(json.dumps(params, indent=2))\n",
        "\n",
        "print('kfold:', fold)\n",
        "print(dataset)\n",
        "print(\"Top words: \",TOP_WORDS)\n",
        "print(\"Embedding size: \", embedding_vector_length)\n",
        "print('bilstm_without_timestamp')\n",
        "print('positive',len(reviews_positive))\n",
        "print('negative',len(reviews_negative))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ShVr9og0jvB"
      },
      "source": [
        "###Code for Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl66t6dVXa6z"
      },
      "source": [
        "####Make directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLy5anab0nKh"
      },
      "source": [
        "!mkdir results/{dataset}/normal/{time_type}/transformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRcE0IkzXsEB"
      },
      "source": [
        "####Hyperparam setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO4dZh7pLqPI"
      },
      "source": [
        "# embedding_vector_length = 30\n",
        "epoch = 350\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "batch_size = 64\n",
        "lr_arr = [0.01, 0.001, 0.0001]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrrmwhj_X9fu"
      },
      "source": [
        "####Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9gaAM_i0njU"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EABnxqyc1ZVg"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnOMOoBy1ZN0"
      },
      "source": [
        "for lr in lr_arr:\n",
        "    !mkdir results/{dataset}/normal/{time_type}/transformer/lr_{lr}\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    path = '/content/drive/MyDrive/Graph_classification/results/'+dataset+'/normal/'+time_type+'/transformer/lr_'+str(lr)+'/'\n",
        "\n",
        "    print(\"\\nEpoch no: \", epoch, \"\\tLearning rate: \", lr)\n",
        "    fold = 0\n",
        "    for train, test in kfold.split(X, Y):\n",
        "        fold = fold+1\n",
        "        filename = path+dataset+'_'+str(fold)+'.csv'\n",
        "        # with open(filename, 'w', newline='') as file:\n",
        "        #     writer = csv.writer(file)\n",
        "        #     writer.writerow([\"roc\", \"prc\", \"val loss\", \"val acc\"])\n",
        "        print('\\nFold no: ', fold)\n",
        "        X_train = X[train]\n",
        "        y_train = Y[train]\n",
        "        X_test = X[test]\n",
        "        y_test = Y[test]\n",
        "        # print(len(X_test))\n",
        "\n",
        "        inputs = layers.Input(shape=(max_len,))\n",
        "        embedding_layer = TokenAndPositionEmbedding(max_len, TOP_WORDS, embed_dim)\n",
        "        x = embedding_layer(inputs)\n",
        "        print(x)\n",
        "        transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "        x = transformer_block(x)\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "        x = layers.Dropout(0.1)(x)\n",
        "        x = layers.Dense(20, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(0.1)(x)\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "        opt = keras.optimizers.Adam(learning_rate=lr)\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                        optimizer=opt, metrics=['accuracy'])\n",
        "        # print(model.summary())\n",
        "        valx = (X_test, y_test)\n",
        "\n",
        "        model.fit(X_train, y_train, \n",
        "            validation_data=(X_test, y_test), epochs=epoch, batch_size=batch_size, callbacks=[PredictionCallback(valx, filename)])\n",
        "\n",
        "    params = {\"kfold\":fold, \"embedding_size\": embed_dim, \"num_heads\": num_heads, \"ff_dim\": ff_dim, \"vocab_size\": len(data), \"max_len\": max_len, \"TOP_WORDS\": TOP_WORDS, \"Epoch\": epoch, \"lr\": lr_arr, \"batch_size\": batch_size, \"Positive\": len(reviews_positive), \"Negative\": len(reviews_negative)}\n",
        "    print(params)\n",
        "\n",
        "    with open ('/content/drive/MyDrive/Graph_classification/results/'+dataset+'/normal/'+time_type+'/transformer/lr_'+str(lr)+'/params.txt', 'w') as f:\n",
        "        f.write(json.dumps(params, indent=2))\n",
        "    save_avg(path)\n",
        "\n",
        "print('kfold:', 5)\n",
        "print(dataset)\n",
        "print(\"Top words: \",TOP_WORDS)\n",
        "print(\"Embedding size: \", embed_dim)\n",
        "print('transformer_without_timestamp')\n",
        "print('positive',len(reviews_positive))\n",
        "print('negative',len(reviews_negative))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}